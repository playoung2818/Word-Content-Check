{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "from flask import Flask, jsonify # type: ignore\n",
    "from flask_sqlalchemy import SQLAlchemy # type: ignore\n",
    "from docx import Document # type: ignore\n",
    "import pdfplumber # type: ignore\n",
    "import pandas as pd # type: ignore\n",
    "import json\n",
    "import sys\n",
    "import requests\n",
    "import re\n",
    "from docx import Document\n",
    "from datetime import datetime, timedelta\n",
    "from sqlalchemy import Column, String, Float, Date, Integer\n",
    "import gspread\n",
    "from gspread_dataframe import set_with_dataframe\n",
    "from oauth2client.service_account import ServiceAccountCredentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_order_file = r\"C:\\Users\\Admin\\OneDrive - neousys-tech\\Share NTA Warehouse\\Daily Update\\Open Sales Order 8_12_2025.CSV\"\n",
    "try:\n",
    "    df_sales_order = pd.read_csv(sales_order_file, encoding=\"ISO-8859-1\")\n",
    "except UnicodeDecodeError:\n",
    "    df_sales_order = pd.read_csv(sales_order_file, encoding=\"latin1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Supabase\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "app.config['SQLALCHEMY_DATABASE_URI'] = (\n",
    "    \"postgresql://postgres.avcznjglmqhmzqtsrlfg:\"\n",
    "    \"Czheyuan0227@aws-0-us-east-2.pooler.supabase.com:6543/postgres\"\n",
    ")\n",
    "app.config['SQLALCHEMY_TRACK_MODIFICATIONS'] = False\n",
    "\n",
    "\n",
    "db = SQLAlchemy()\n",
    "\n",
    "# Hook your existing db object up to this app\n",
    "db.init_app(app)\n",
    "\n",
    "class ReceivingLog(db.Model):\n",
    "    __tablename__   = 'receiving_log'\n",
    "    id = Column(Integer, primary_key=True, autoincrement=True)\n",
    "    serial_number   = db.Column(db.String(255), primary_key=True)\n",
    "    entry_date      = db.Column(db.Date)\n",
    "    invoice_number  = db.Column(db.String(255))\n",
    "    box_number      = db.Column(db.String(255))\n",
    "    pod_number      = db.Column(db.String(255))\n",
    "    part_number     = db.Column(db.String(255))\n",
    "    quantity        = db.Column(db.Float)\n",
    "    reference     = db.Column('Reference', db.Text)  \n",
    "\n",
    "def extract_useful_number(sn: str) -> str:\n",
    "    \"\"\"\n",
    "    Extracts the serial number portion from a string in the format 'SN########'.\n",
    "    \"\"\"\n",
    "    match = re.search(r\"SN?([A-Za-z0-9\\-]+)\", sn)\n",
    "    return match.group(1) if match else sn.strip()\n",
    "\n",
    "def get_part_name_from_db(serial_number: str):\n",
    "    try:\n",
    "        with app.app_context():\n",
    "            entry = ReceivingLog.query.filter_by(serial_number=serial_number).first()\n",
    "            return entry.part_number if entry else None\n",
    "    except Exception as e:\n",
    "        logging.error(f\"DB error for {serial_number}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def extract_product_details_from_word(file_path):\n",
    "    try:\n",
    "        document = Document(file_path)\n",
    "        if not document.tables:\n",
    "            return []\n",
    "\n",
    "        table = document.tables[0]\n",
    "        product_details = []\n",
    "        for row in table.rows[1:-1]:\n",
    "            cells = row.cells\n",
    "            if len(cells) < 4:\n",
    "                continue\n",
    "\n",
    "            sn_text = cells[2].text.strip()\n",
    "            if sn_text.upper() in {\"NA\", \"N/A\", \"NONE\"} or not sn_text.strip():\n",
    "                continue  \n",
    "\n",
    "            product_details.append({\n",
    "                \"product_number\": cells[0].text.strip(),\n",
    "                \"qty\": cells[1].text.strip(),\n",
    "                \"sn\": cells[2].text.strip(),\n",
    "                \"notes\": cells[3].text.strip()\n",
    "            })\n",
    "        return product_details\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading Word file {file_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "def validate_sn_part_matches_via_api(file_path):\n",
    "    \"\"\"\n",
    "    Validates if the serial numbers from the Word file match the database.\n",
    "    Also checks if the quantity matches the number of serial numbers.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    product_details = extract_product_details_from_word(file_path)\n",
    "\n",
    "    for item in product_details:\n",
    "        word_part = item[\"product_number\"]\n",
    "        sn_block = item[\"sn\"]\n",
    "        qty_text = item[\"qty\"]\n",
    "        serials = [s.strip() for s in sn_block.split('\\n') if s.strip() and s.strip().upper() not in {\"NA\", \"N/A\"}]\n",
    "        sn_count = len(serials)\n",
    "\n",
    "        try:\n",
    "            expected_qty = int(qty_text)\n",
    "        except ValueError:\n",
    "            expected_qty = None\n",
    "\n",
    "        qty_match = (sn_count == expected_qty) if expected_qty is not None else False\n",
    "\n",
    "        if not serials:\n",
    "            results.append({\n",
    "                \"serial_number\": \"N/A\",\n",
    "                \"word_part\": word_part,\n",
    "                \"db_part\": None,\n",
    "                \"status\": \"❓ NOT FOUND\",\n",
    "                \"qty_check\": f\"❌ Qty Mismatch\"\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        for sn in serials:\n",
    "            db_part = get_part_name_from_db(sn)\n",
    "            if db_part:\n",
    "                match = \"✅ MATCH\" if db_part == word_part else \"❌ MISMATCH\"\n",
    "            else:\n",
    "                match = \"❓ NOT FOUND\"\n",
    "\n",
    "            results.append({\n",
    "                \"serial_number\": sn,\n",
    "                \"word_part\": word_part,\n",
    "                \"db_part\": db_part,\n",
    "                \"status\": match,\n",
    "                \"qty_check\": f\"✅ Qty OK\" if qty_match else f\"❌ Qty Mismatch (Expected {expected_qty}, Found {sn_count})\"\n",
    "            })\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def load_receiving_log(path_to_xlsm: str, engine, dry_run=False):\n",
    "    import pandas as pd\n",
    "\n",
    "    # Load only the 'Receiving' sheet\n",
    "    df = pd.read_excel(path_to_xlsm, sheet_name=\"Receiving\")\n",
    "\n",
    "    # Remove rows where all critical fields are blank (before renaming)\n",
    "    df = df.dropna(subset=['Date', 'Inv# ', 'Box #', 'POD#', 'Part#', 'SN#'], how='all')\n",
    "\n",
    "    # Rename columns to standardized names\n",
    "    df.rename(columns={\n",
    "        'Date': 'entry_date',\n",
    "        'Inv# ': 'invoice_number',\n",
    "        'Box #': 'box_number',\n",
    "        'POD#': 'pod_number',\n",
    "        'Part#': 'part_number',\n",
    "        'SN#': 'serial_number',\n",
    "        'QTY': 'quantity'\n",
    "    }, inplace=True)\n",
    "\n",
    "    # Normalize types and strip spaces\n",
    "    df['quantity'] = df['quantity'].fillna(1).astype(float)\n",
    "    df['entry_date'] = pd.to_datetime(df['entry_date'], errors='coerce')\n",
    "    df['serial_number'] = df['serial_number'].astype(str).str.strip().replace(\"nan\", \"NA\")\n",
    "\n",
    "    string_cols = ['invoice_number', 'box_number', 'pod_number', 'part_number', 'serial_number']\n",
    "    for col in string_cols:\n",
    "        df[col] = df[col].astype(str).str.strip()\n",
    "\n",
    "    # Print preview after cleanup\n",
    "    print(f\"🧾 Cleaned DataFrame Preview:\\n{df.tail(25)}\")\n",
    "\n",
    "    # Load existing data for deduplication\n",
    "    existing = pd.read_sql(\"\"\"\n",
    "        SELECT entry_date, invoice_number, box_number, pod_number, part_number, serial_number, quantity\n",
    "        FROM receiving_log\n",
    "    \"\"\", engine)\n",
    "\n",
    "    existing['entry_date'] = pd.to_datetime(existing['entry_date'], errors='coerce')\n",
    "    for col in string_cols:\n",
    "        existing[col] = existing[col].astype(str).str.strip()\n",
    "    existing['quantity'] = existing['quantity'].astype(float)\n",
    "\n",
    "\n",
    "    # 🔍 Deduplicate based on key fields\n",
    "    key_cols = ['entry_date', 'invoice_number', 'box_number', 'pod_number', 'part_number', 'serial_number', 'quantity']\n",
    "    merged = df.merge(existing, how='left', indicator=True, on=key_cols)\n",
    "    print(f\"🧾 merged:\\n{merged.tail(25)}\")\n",
    "    new_rows = merged[merged['_merge'] == 'left_only'].drop(columns=['_merge'])\n",
    "\n",
    "    print(f\"🟡 Dry Run: {len(new_rows)} new rows would be inserted (out of {len(df)} total).\")\n",
    "\n",
    "    if not dry_run:\n",
    "        new_rows.to_sql('receiving_log', engine, if_exists='append', index=False, method='multi')\n",
    "        print(\"✅ Data inserted.\")\n",
    "    else:\n",
    "        print(\"🚫 Dry run mode — no data inserted.\")\n",
    "        print(\"🔍 Preview of rows to be inserted:\")\n",
    "        print(new_rows.head(10))\n",
    "\n",
    "def extract_wo_from_filename(filename):\n",
    "    # Match either WO-20250970 or WO07-20250970\n",
    "    match = re.search(r\"WO\\d{2}-(\\d{8})\", filename)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    return None\n",
    "\n",
    "def validate_part_number_and_qty(file_path, df_sales_order):\n",
    "    \"\"\"\n",
    "    Validates part numbers and quantities from a Word file against a filtered sales order.\n",
    "    \"\"\"\n",
    "    word_parts = extract_product_details_from_word(file_path)\n",
    "    filename = file_path.split(\"\\\\\")[-1]\n",
    "    wo_number = f'SO-{extract_wo_from_filename(filename)}'\n",
    "\n",
    "    if not wo_number:\n",
    "        print(f\"❌ Could not extract WO number from filename: {filename}\")\n",
    "        return []\n",
    "\n",
    "    df_filtered = df_sales_order[df_sales_order[\"WO_Number\"] == wo_number]\n",
    "    if df_filtered.empty:\n",
    "        print(f\"❌ No matching WO_Number found in sales order: {wo_number}\")\n",
    "        return []\n",
    "\n",
    "    results = []\n",
    "    for item in word_parts:\n",
    "        word_part = item[\"product_number\"]\n",
    "        qty_text = item[\"qty\"]\n",
    "\n",
    "        try:\n",
    "            expected_qty = int(qty_text)\n",
    "        except ValueError:\n",
    "            expected_qty = None\n",
    "\n",
    "        matched_rows = df_filtered[\n",
    "            df_filtered[\"Component\"].apply(normalize) == normalize(word_part)\n",
    "        ]\n",
    "\n",
    "        if matched_rows.empty:\n",
    "            results.append({\n",
    "                \"word_part\": word_part,\n",
    "                \"status\": \"❓ NOT FOUND\",\n",
    "                \"required_qty\": None,\n",
    "                \"word_qty\": expected_qty,\n",
    "                \"qty_match\": \"❌ N/A\",\n",
    "                \"WO_Number\": wo_number\n",
    "            })\n",
    "        else:\n",
    "            db_qty = matched_rows[\"Required_Qty\"].iloc[0]\n",
    "            qty_match = expected_qty == db_qty if expected_qty is not None else False\n",
    "            results.append({\n",
    "                \"word_part\": word_part,\n",
    "                \"status\": \"✅ MATCH\" if qty_match else \"❌ QTY MISMATCH\",\n",
    "                \"required_qty\": db_qty,\n",
    "                \"word_qty\": expected_qty,\n",
    "                \"qty_match\": f\"✅ Qty OK\" if qty_match else f\"❌ Expected {db_qty}, Got {expected_qty}\",\n",
    "                \"WO_Number\": wo_number\n",
    "            })\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "## Parse PDF then upload to google sheet\n",
    "\n",
    "def has_dash_comments(pdf_path):\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            if not hasattr(page, \"annots\") or page.annots is None:\n",
    "                continue\n",
    "            for annot in page.annots:\n",
    "                content = annot.get(\"contents\") or \"\"\n",
    "                if \"-\" in content:\n",
    "                    return True\n",
    "    return False\n",
    "\n",
    "def extract_pdf_data_with_dash_check(pdf_path):\n",
    "    filename = os.path.basename(pdf_path).replace('.pdf', '')\n",
    "\n",
    "    # Extract WO\n",
    "    wo_match = re.search(r\"WO-(\\d{8}(?:r\\d*)?)\", filename)\n",
    "    wo = wo_match.group(1) if wo_match else None\n",
    "\n",
    "    # Extract PO\n",
    "    po_match = re.search(r\"(PO[#_])(\\w+)\", filename)\n",
    "    po = po_match.group(2) if po_match else None\n",
    "\n",
    "    # Extract customer\n",
    "    customer = None\n",
    "    if wo_match and po_match:\n",
    "        try:\n",
    "            customer_raw = filename.split(f\"WO-{wo}_\")[1].split(po_match.group(0))[0]\n",
    "            customer = customer_raw.strip('_ ').replace('_', ' ')\n",
    "        except Exception:\n",
    "            customer = None\n",
    "\n",
    "    dash_flag = has_dash_comments(pdf_path)\n",
    "\n",
    "    if not all([wo, customer, po]):\n",
    "        return wo, customer, po, pd.DataFrame(), dash_flag\n",
    "\n",
    "    # Extract tables\n",
    "    tables = []\n",
    "    cleaned_header = ['Item', 'Description', 'Site', 'Ordered', 'To Pick']\n",
    "\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            table = page.extract_table()\n",
    "            if not table:\n",
    "                continue\n",
    "            header = table[0]\n",
    "            cleaned_raw_header = [str(col).strip() if col else \"\" for col in header]\n",
    "\n",
    "            if cleaned_raw_header == cleaned_header:\n",
    "                df = pd.DataFrame(table[1:], columns=cleaned_header)\n",
    "                tables.append(df)\n",
    "            elif 'P.O. No.' in cleaned_raw_header:\n",
    "                try:\n",
    "                    po_index = cleaned_raw_header.index('P.O. No.')\n",
    "                    data_rows = [row[:po_index] + row[po_index+1:] for row in table[1:]]\n",
    "                    cleaned_alt_header = cleaned_raw_header[:po_index] + cleaned_raw_header[po_index+1:]\n",
    "                    if len(cleaned_header) == len(cleaned_alt_header):\n",
    "                        df = pd.DataFrame(data_rows, columns=cleaned_header)\n",
    "                        tables.append(df)\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "    if not tables:\n",
    "        return wo, customer, po, pd.DataFrame(), dash_flag\n",
    "\n",
    "    # Clean\n",
    "    final_df = pd.concat(tables, ignore_index=True)\n",
    "    final_df.columns = final_df.columns.str.strip()\n",
    "    final_df = final_df[final_df['Site'].astype(str).str.startswith('WH')].copy()\n",
    "\n",
    "    def extract_or_default(row):\n",
    "        col1 = row['Item']\n",
    "        col2 = row['Description']\n",
    "\n",
    "        def clean(text):\n",
    "            return (\n",
    "                text.strip()\n",
    "                    .lstrip('*')\n",
    "                    .replace(\"(cid:95)\", \"_\")\n",
    "                    .replace(\"(cid:74)\", \"J\")\n",
    "            )\n",
    "\n",
    "        if isinstance(col1, str) and '...' in col1:\n",
    "            if isinstance(col2, str):\n",
    "                part = col2.strip().split('\\n')[0].strip()\n",
    "                return clean(part)\n",
    "            return None\n",
    "        elif isinstance(col1, str):\n",
    "            return clean(col1)\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    final_df['Extracted'] = final_df.apply(extract_or_default, axis=1)\n",
    "    return wo, customer, po, final_df[['Extracted', 'Ordered']], dash_flag\n",
    "\n",
    "def process_all_pdfs(folder_path):\n",
    "    all_rows = []\n",
    "\n",
    "    for root, _, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if file.lower().endswith(\".pdf\"):\n",
    "                pdf_path = os.path.join(root, file)\n",
    "                wo, customer, po, df, dash_flag = extract_pdf_data_with_dash_check(pdf_path)\n",
    "                if df.empty:\n",
    "                    continue\n",
    "                for _, row in df.iterrows():\n",
    "                    all_rows.append({\n",
    "                        \"Customer\": customer,\n",
    "                        \"PO\": po,\n",
    "                        \"WO\": f\"SO-{wo}\",\n",
    "                        \"Product Number\": row['Extracted'],\n",
    "                        \"Qty\": row['Ordered'],\n",
    "                        \"Source File\": file,\n",
    "                        \"Flagged\": \"⚠️\" if dash_flag else \"\"\n",
    "                    })\n",
    "\n",
    "    return pd.DataFrame(all_rows)\n",
    "\n",
    "def write_df_to_gsheet(df, sheet_name, worksheet_name, cred_path=\"path/to/your/credentials.json\"):\n",
    "    scope = ['https://spreadsheets.google.com/feeds', 'https://www.googleapis.com/auth/drive']\n",
    "    creds = ServiceAccountCredentials.from_json_keyfile_name(cred_path, scope)\n",
    "    client = gspread.authorize(creds)\n",
    "\n",
    "    sheet = client.open(sheet_name)\n",
    "    try:\n",
    "        worksheet = sheet.worksheet(worksheet_name)\n",
    "        worksheet.clear()\n",
    "    except gspread.exceptions.WorksheetNotFound:\n",
    "        worksheet = sheet.add_worksheet(title=worksheet_name, rows=1000, cols=20)\n",
    "\n",
    "    set_with_dataframe(worksheet, df)\n",
    "    print(f\"✅ Data written to Google Sheet: {sheet_name} [{worksheet_name}]\")\n",
    "\n",
    "def components_for_wo(df, wo):\n",
    "    # wo_norm = re.sub(r'^SO-', '', str(wo)).strip()\n",
    "    wo_norm = wo\n",
    "    return df[df['WO'] == wo_norm].sort_values('Product Number').reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Parse PDF. Merge PDF with Open Sales Order, keep Open Sales Order, Order by PDF\n",
    "def reorder_df_out_by_output(output_df: pd.DataFrame, df_out: pd.DataFrame) -> pd.DataFrame:\n",
    "    # 1) Build the reference order from output_df (the order you trust)\n",
    "    ref = output_df.copy()\n",
    "    ref['__pos_out'] = ref.groupby('WO').cumcount()                 # position within WO\n",
    "    ref['__occ'] = ref.groupby(['WO','Product Number']).cumcount()  # occurrence index for duplicates\n",
    "    ref_key = ref[['WO','Product Number','__occ','__pos_out']]\n",
    "\n",
    "    # 2) On df_out, tag each duplicate with its own occurrence index\n",
    "    tgt = df_out.copy()\n",
    "    tgt['__occ'] = tgt.groupby(['WO','Product Number']).cumcount()\n",
    "\n",
    "    # 3) Merge positions from output_df to df_out rows (match by WO + Product + occurrence)\n",
    "    merged = tgt.merge(ref_key, on=['WO','Product Number','__occ'], how='left')\n",
    "\n",
    "    # 4) For rows not present in output_df, keep their original within-WO order but push them after the matched ones\n",
    "    merged['__fallback'] = merged.groupby('WO').cumcount()\n",
    "    merged['__pos_out'] = merged['__pos_out'].fillna(np.inf)\n",
    "\n",
    "    # 5) Final order: by WO, then by output_df position; if missing, by original order\n",
    "    ordered = (merged\n",
    "               .sort_values(['WO','__pos_out','__fallback'])\n",
    "               .drop(columns=['__occ','__pos_out','__fallback'])\n",
    "               .reset_index(drop=True))\n",
    "    return ordered\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run\n",
    "folder = r\"\\\\Quickbook2024\\d\\Drive D\\QuickBooks\\3- Year 2025\\Work Order- WO\"\n",
    "google_cred_path = r\"C:\\Users\\Admin\\Downloads\\pdfwo-466115-734096e1cef8.json\"\n",
    "\n",
    "output_df = process_all_pdfs(folder)\n",
    "\n",
    "if not output_df.empty:\n",
    "    write_df_to_gsheet(\n",
    "        output_df,\n",
    "        sheet_name=\"PDF_WO\",\n",
    "        worksheet_name=\"Sheet1\",\n",
    "        cred_path=google_cred_path\n",
    "    )\n",
    "else:\n",
    "    print(\"❌ No output to write.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse Open Sales Order.CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Data written to Google Sheet: PDF_WO [Open Sales Order]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Customer</th>\n",
       "      <th>PO</th>\n",
       "      <th>WO</th>\n",
       "      <th>Product Number</th>\n",
       "      <th>Qty</th>\n",
       "      <th>Source File</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>Aerflite Canada Inc.</td>\n",
       "      <td>564</td>\n",
       "      <td>SO-20250494</td>\n",
       "      <td>CS-RM44-601</td>\n",
       "      <td>10.0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>Aerflite Canada Inc.</td>\n",
       "      <td>564</td>\n",
       "      <td>SO-20250494</td>\n",
       "      <td>LC-XE360-SP5-601</td>\n",
       "      <td>10.0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>Aerflite Canada Inc.</td>\n",
       "      <td>564</td>\n",
       "      <td>SO-20250494</td>\n",
       "      <td>PA-HA1300R-PM-601</td>\n",
       "      <td>10.0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>Aerflite Canada Inc.</td>\n",
       "      <td>564</td>\n",
       "      <td>SO-20250494</td>\n",
       "      <td>RDDR5-32GB-ECC48WT-TD-601</td>\n",
       "      <td>40.0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>Aerflite Canada Inc.</td>\n",
       "      <td>564</td>\n",
       "      <td>SO-20250494</td>\n",
       "      <td>GC-RTX6000Ada-PNY-601</td>\n",
       "      <td>10.0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>Aerflite Canada Inc.</td>\n",
       "      <td>564</td>\n",
       "      <td>SO-20250494</td>\n",
       "      <td>Engineer Services</td>\n",
       "      <td>10.0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>Aerflite Canada Inc.</td>\n",
       "      <td>564</td>\n",
       "      <td>SO-20250494</td>\n",
       "      <td>MB-GENOAD8X-2T/BCM-9534-SSD-601</td>\n",
       "      <td>10.0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Customer   PO           WO                   Product Number  \\\n",
       "151  Aerflite Canada Inc.  564  SO-20250494                      CS-RM44-601   \n",
       "152  Aerflite Canada Inc.  564  SO-20250494                 LC-XE360-SP5-601   \n",
       "153  Aerflite Canada Inc.  564  SO-20250494                PA-HA1300R-PM-601   \n",
       "154  Aerflite Canada Inc.  564  SO-20250494        RDDR5-32GB-ECC48WT-TD-601   \n",
       "155  Aerflite Canada Inc.  564  SO-20250494            GC-RTX6000Ada-PNY-601   \n",
       "156  Aerflite Canada Inc.  564  SO-20250494                Engineer Services   \n",
       "157  Aerflite Canada Inc.  564  SO-20250494  MB-GENOAD8X-2T/BCM-9534-SSD-601   \n",
       "\n",
       "      Qty Source File  \n",
       "151  10.0              \n",
       "152  10.0              \n",
       "153  10.0              \n",
       "154  40.0              \n",
       "155  10.0              \n",
       "156  10.0              \n",
       "157  10.0              "
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sales_order.rename(columns={\n",
    "    'Unnamed: 0': 'Component',\n",
    "    'Num': 'WO_Number',\n",
    "    'Qty': 'Required_Qty'\n",
    "}, inplace=True)\n",
    "\n",
    "df_sales_order['Component'] = df_sales_order['Component'].ffill().astype(str).str.strip()\n",
    "df_sales_order = df_sales_order[~df_sales_order[\"Component\"].str.startswith(\"total\")]\n",
    "# df_sales_order[\"WO_Number\"] = (\n",
    "#     df_sales_order[\"WO_Number\"].astype(str).str.strip().str.replace(r'^SO-', '', regex=True)\n",
    "# )\n",
    "df_sales_order = df_sales_order[\n",
    "    ~df_sales_order[\"Component\"].str.lower().isin([\"forwarding charge\", \"tariff (estimation)\"])\n",
    "]\n",
    "\n",
    "# ---------- Format like your screenshot ----------\n",
    "# If your CSV already has Customer / PO / Source File columns, these will pass through.\n",
    "# If not, we create empty placeholders so the table layout matches.\n",
    "needed_cols = {\n",
    "    'Name': 'Customer',\n",
    "    'P. O. #': 'PO',\n",
    "    'WO_Number': 'WO',\n",
    "    'Component': 'Product Number',\n",
    "    'Required_Qty': 'Qty',\n",
    "    'Source File': 'Source File'\n",
    "}\n",
    "for c in ['Customer', 'PO', 'Source File']:\n",
    "    if c not in df_sales_order.columns:\n",
    "        df_sales_order[c] = \"\"\n",
    "\n",
    "df_out = df_sales_order.rename(columns=needed_cols)[list(needed_cols.values())]\n",
    "\n",
    "# Sort to group visually by WO, then by Product Number\n",
    "df_out = df_out.sort_values(['WO', 'Product Number']).reset_index(drop=True)\n",
    "\n",
    "# Usage:\n",
    "final_df = reorder_df_out_by_output(output_df, df_out)\n",
    "\n",
    "\n",
    "# your specific mappings (only exact string-to-string)\n",
    "mappings = {\n",
    "    'M.280-SSD-256GB-PCIe44-TLC5WT-T': 'M.280-SSD-256GB-PCIe44-TLC5WT-TD',\n",
    "    'M.280-SSD-512GB-PCIe44-TLC5WT-T': 'M.280-SSD-512GB-PCIe44-TLC5WT-TD',\n",
    "    'M.242-SSD-256GB-PCIe34-TLC5WT-T': 'M.242-SSD-256GB-PCIe34-TLC5WT-TD',\n",
    "    'M.242-SSD-512GB-PCIe34-TLC5WT-T': 'M.242-SSD-512GB-PCIe34-TLC5WT-TD',\n",
    "    'Cblkit-FP-NRU-230V-AWP_NRU-240S': 'Cblkit-FP-NRU-230V-AWP_NRU-240S-AWP',\n",
    "}\n",
    "\n",
    "final_df['Product Number'] = final_df['Product Number'].replace(mappings)\n",
    "\n",
    "final_df = final_df.loc[:, ~final_df.columns.duplicated()]\n",
    "\n",
    "# Upload to Google Sheet\n",
    "google_cred_path = r\"C:\\Users\\Admin\\Downloads\\pdfwo-466115-734096e1cef8.json\"\n",
    "\n",
    "if not final_df.empty:\n",
    "    write_df_to_gsheet(\n",
    "        final_df,\n",
    "        sheet_name=\"PDF_WO\",\n",
    "        worksheet_name=\"Open Sales Order\",\n",
    "        cred_path=google_cred_path\n",
    "    )\n",
    "else:\n",
    "    print(\"❌ No output to write.\")\n",
    "\n",
    "\n",
    "# ✅ Now final_df has Product Number overridden from df_out when WO matches and value differs\n",
    "final_df[final_df['WO'] == 'SO-20250494']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Receiving Log into DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧾 Cleaned DataFrame Preview:\n",
      "     entry_date   invoice_number box_number  pod_number part_number  \\\n",
      "2042 2025-08-11  Newegg Business       1--1  POD-251120    i7-14700   \n",
      "2043 2025-08-11  Newegg Business       1--1  POD-251120    i7-14700   \n",
      "2044 2025-08-11  Newegg Business       1--1  POD-251120    i7-14700   \n",
      "2045 2025-08-11  Newegg Business       1--1  POD-251120    i7-14700   \n",
      "2046 2025-08-11  Newegg Business       1--1  POD-251120    i7-14700   \n",
      "2047 2025-08-11  Newegg Business       1--1  POD-251120    i7-14700   \n",
      "2048 2025-08-11  Newegg Business       1--1  POD-251120    i7-14700   \n",
      "2049 2025-08-11  Newegg Business       1--1  POD-251120    i7-14700   \n",
      "2050 2025-08-11  Newegg Business       1--1  POD-251120    i7-14700   \n",
      "2051 2025-08-11  Newegg Business       1--1  POD-251120    i7-14700   \n",
      "2052 2025-08-11  Newegg Business       1--1  POD-251120    i7-14700   \n",
      "2053 2025-08-11  Newegg Business       1--1  POD-251120    i7-14700   \n",
      "2054 2025-08-11  Newegg Business       1--1  POD-251120    i7-14700   \n",
      "2055 2025-08-11  Newegg Business       1--1  POD-251120    i7-14700   \n",
      "2056 2025-08-11  Newegg Business       1--1  POD-251120    i7-14700   \n",
      "2057 2025-08-11  Newegg Business       1--1  POD-251120    i7-14700   \n",
      "2058 2025-08-11  Newegg Business       1--1  POD-251120    i7-14700   \n",
      "2059 2025-08-11  Newegg Business       1--1  POD-251120    i7-14700   \n",
      "2060 2025-08-11  Newegg Business       1--1  POD-251120    i7-14700   \n",
      "2061 2025-08-11  Newegg Business       1--1  POD-251120    i7-14700   \n",
      "2062 2025-08-11  Newegg Business       1--1  POD-251120    i7-14700   \n",
      "2063 2025-08-11  Newegg Business       1--1  POD-251120    i7-14700   \n",
      "2064 2025-08-11  Newegg Business       1--1  POD-251120    i7-14700   \n",
      "2065 2025-08-11  Newegg Business       1--1  POD-251120    i7-14700   \n",
      "2066 2025-08-11  Newegg Business       1--1  POD-251120    i7-14700   \n",
      "\n",
      "                serial_number  quantity  \\\n",
      "2042  U5J41S4301311, X519K773       1.0   \n",
      "2043  U56E59H803140, X517L194       1.0   \n",
      "2044  U52R3F1604268, X519K773       1.0   \n",
      "2045  U59E4L2004155, X520M299       1.0   \n",
      "2046  U5WH745603445, X521J319       1.0   \n",
      "2047  U5P39Q2703551, X522L149       1.0   \n",
      "2048  U5VQ291404960, X521J319       1.0   \n",
      "2049  U59E4L2004374, X520M299       1.0   \n",
      "2050  U5AE243803382, X517L194       1.0   \n",
      "2051  U522EA8300810, X522L149       1.0   \n",
      "2052  U5AE243805519, X517L194       1.0   \n",
      "2053  U5AE243804110, X517L194       1.0   \n",
      "2054  U5P39Q2702553, X522L149       1.0   \n",
      "2055  U5AE243805526, X517L194       1.0   \n",
      "2056  U5AE243804111, X517L194       1.0   \n",
      "2057  U5AE243802793, X517L194       1.0   \n",
      "2058  U5YE411301578, X517L194       1.0   \n",
      "2059  U5AE243804392, X517L194       1.0   \n",
      "2060  U5YE411304959, X517L194       1.0   \n",
      "2061  U5AE243805157, X517L194       1.0   \n",
      "2062  U5AE243805501, X517L194       1.0   \n",
      "2063  U5AE243804991, X517L194       1.0   \n",
      "2064  U5YE411301608, X517L194       1.0   \n",
      "2065  U5AE243803568, X517L194       1.0   \n",
      "2066  U5AE243803732, X517L194       1.0   \n",
      "\n",
      "                                          Reference  \n",
      "2042  Newegg Business_i7-14700 (x25)_i9-14900 (x10)  \n",
      "2043  Newegg Business_i7-14700 (x25)_i9-14900 (x10)  \n",
      "2044  Newegg Business_i7-14700 (x25)_i9-14900 (x10)  \n",
      "2045  Newegg Business_i7-14700 (x25)_i9-14900 (x10)  \n",
      "2046                                            NaN  \n",
      "2047                                            NaN  \n",
      "2048                                            NaN  \n",
      "2049                                            NaN  \n",
      "2050                                            NaN  \n",
      "2051                                            NaN  \n",
      "2052                                            NaN  \n",
      "2053                                            NaN  \n",
      "2054                                            NaN  \n",
      "2055                                            NaN  \n",
      "2056                                            NaN  \n",
      "2057                                            NaN  \n",
      "2058                                            NaN  \n",
      "2059                                            NaN  \n",
      "2060                                            NaN  \n",
      "2061                                            NaN  \n",
      "2062                                            NaN  \n",
      "2063                                            NaN  \n",
      "2064                                            NaN  \n",
      "2065                                            NaN  \n",
      "2066                                            NaN  \n",
      "🧾 merged:\n",
      "     entry_date   invoice_number box_number  pod_number part_number  \\\n",
      "2058 2025-08-11  Newegg Business       1--1  POD-251120    i7-14700   \n",
      "2059 2025-08-11  Newegg Business       1--1  POD-251120    i7-14700   \n",
      "2060 2025-08-11  Newegg Business       1--1  POD-251120    i7-14700   \n",
      "2061 2025-08-11  Newegg Business       1--1  POD-251120    i7-14700   \n",
      "2062 2025-08-11  Newegg Business       1--1  POD-251120    i7-14700   \n",
      "2063 2025-08-11  Newegg Business       1--1  POD-251120    i7-14700   \n",
      "2064 2025-08-11  Newegg Business       1--1  POD-251120    i7-14700   \n",
      "2065 2025-08-11  Newegg Business       1--1  POD-251120    i7-14700   \n",
      "2066 2025-08-11  Newegg Business       1--1  POD-251120    i7-14700   \n",
      "2067 2025-08-11  Newegg Business       1--1  POD-251120    i7-14700   \n",
      "2068 2025-08-11  Newegg Business       1--1  POD-251120    i7-14700   \n",
      "2069 2025-08-11  Newegg Business       1--1  POD-251120    i7-14700   \n",
      "2070 2025-08-11  Newegg Business       1--1  POD-251120    i7-14700   \n",
      "2071 2025-08-11  Newegg Business       1--1  POD-251120    i7-14700   \n",
      "2072 2025-08-11  Newegg Business       1--1  POD-251120    i7-14700   \n",
      "2073 2025-08-11  Newegg Business       1--1  POD-251120    i7-14700   \n",
      "2074 2025-08-11  Newegg Business       1--1  POD-251120    i7-14700   \n",
      "2075 2025-08-11  Newegg Business       1--1  POD-251120    i7-14700   \n",
      "2076 2025-08-11  Newegg Business       1--1  POD-251120    i7-14700   \n",
      "2077 2025-08-11  Newegg Business       1--1  POD-251120    i7-14700   \n",
      "2078 2025-08-11  Newegg Business       1--1  POD-251120    i7-14700   \n",
      "2079 2025-08-11  Newegg Business       1--1  POD-251120    i7-14700   \n",
      "2080 2025-08-11  Newegg Business       1--1  POD-251120    i7-14700   \n",
      "2081 2025-08-11  Newegg Business       1--1  POD-251120    i7-14700   \n",
      "2082 2025-08-11  Newegg Business       1--1  POD-251120    i7-14700   \n",
      "\n",
      "                serial_number  quantity  \\\n",
      "2058  U5J41S4301311, X519K773       1.0   \n",
      "2059  U56E59H803140, X517L194       1.0   \n",
      "2060  U52R3F1604268, X519K773       1.0   \n",
      "2061  U59E4L2004155, X520M299       1.0   \n",
      "2062  U5WH745603445, X521J319       1.0   \n",
      "2063  U5P39Q2703551, X522L149       1.0   \n",
      "2064  U5VQ291404960, X521J319       1.0   \n",
      "2065  U59E4L2004374, X520M299       1.0   \n",
      "2066  U5AE243803382, X517L194       1.0   \n",
      "2067  U522EA8300810, X522L149       1.0   \n",
      "2068  U5AE243805519, X517L194       1.0   \n",
      "2069  U5AE243804110, X517L194       1.0   \n",
      "2070  U5P39Q2702553, X522L149       1.0   \n",
      "2071  U5AE243805526, X517L194       1.0   \n",
      "2072  U5AE243804111, X517L194       1.0   \n",
      "2073  U5AE243802793, X517L194       1.0   \n",
      "2074  U5YE411301578, X517L194       1.0   \n",
      "2075  U5AE243804392, X517L194       1.0   \n",
      "2076  U5YE411304959, X517L194       1.0   \n",
      "2077  U5AE243805157, X517L194       1.0   \n",
      "2078  U5AE243805501, X517L194       1.0   \n",
      "2079  U5AE243804991, X517L194       1.0   \n",
      "2080  U5YE411301608, X517L194       1.0   \n",
      "2081  U5AE243803568, X517L194       1.0   \n",
      "2082  U5AE243803732, X517L194       1.0   \n",
      "\n",
      "                                          Reference     _merge  \n",
      "2058  Newegg Business_i7-14700 (x25)_i9-14900 (x10)  left_only  \n",
      "2059  Newegg Business_i7-14700 (x25)_i9-14900 (x10)  left_only  \n",
      "2060  Newegg Business_i7-14700 (x25)_i9-14900 (x10)  left_only  \n",
      "2061  Newegg Business_i7-14700 (x25)_i9-14900 (x10)  left_only  \n",
      "2062                                            NaN  left_only  \n",
      "2063                                            NaN  left_only  \n",
      "2064                                            NaN  left_only  \n",
      "2065                                            NaN  left_only  \n",
      "2066                                            NaN  left_only  \n",
      "2067                                            NaN  left_only  \n",
      "2068                                            NaN  left_only  \n",
      "2069                                            NaN  left_only  \n",
      "2070                                            NaN  left_only  \n",
      "2071                                            NaN  left_only  \n",
      "2072                                            NaN  left_only  \n",
      "2073                                            NaN  left_only  \n",
      "2074                                            NaN  left_only  \n",
      "2075                                            NaN  left_only  \n",
      "2076                                            NaN  left_only  \n",
      "2077                                            NaN  left_only  \n",
      "2078                                            NaN  left_only  \n",
      "2079                                            NaN  left_only  \n",
      "2080                                            NaN  left_only  \n",
      "2081                                            NaN  left_only  \n",
      "2082                                            NaN  left_only  \n",
      "🟡 Dry Run: 29 new rows would be inserted (out of 2067 total).\n",
      "✅ Data inserted.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # 1️⃣ Push the Flask context so db.engine is wired up:\n",
    "    with app.app_context():\n",
    "        # 2️⃣ Ensure the receiving_log table exists\n",
    "        db.create_all()\n",
    "\n",
    "        # 3️⃣ Now call your loader, handing it db.engine\n",
    "        load_receiving_log(\n",
    "            r\"C:\\Users\\Admin\\OneDrive - neousys-tech\\Share NTA Warehouse\\01 Incoming\\Receiving Log_ZC_2.0.xlsm\",\n",
    "            db.engine\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Go Through Today's WOs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:\n",
      "📄 Processing file: WO08-20251009-American Portwell Technology, Inc..docx\n",
      "INFO:root:🔍 Serial Number Validation:\n",
      "INFO:root:SN: P2400022 | Word Part: PCIe-N572 | DB Part: PCIe-N572 | Status: ✅ MATCH | Qty Check: ✅ Qty OK\n",
      "INFO:root:SN: P2400023 | Word Part: PCIe-N572 | DB Part: PCIe-N572 | Status: ✅ MATCH | Qty Check: ✅ Qty OK\n",
      "INFO:root:SN: P2600525 | Word Part: PCIe-N572 | DB Part: PCIe-N572 | Status: ✅ MATCH | Qty Check: ✅ Qty OK\n",
      "INFO:root:SN: P2600526 | Word Part: PCIe-N572 | DB Part: PCIe-N572 | Status: ✅ MATCH | Qty Check: ✅ Qty OK\n",
      "INFO:root:SN: P2800351 | Word Part: PCIe-N572 | DB Part: PCIe-N572 | Status: ✅ MATCH | Qty Check: ✅ Qty OK\n",
      "INFO:root:SN: P2800352 | Word Part: PCIe-N572 | DB Part: PCIe-N572 | Status: ✅ MATCH | Qty Check: ✅ Qty OK\n",
      "INFO:root:SN: P2800353 | Word Part: PCIe-N572 | DB Part: PCIe-N572 | Status: ✅ MATCH | Qty Check: ✅ Qty OK\n",
      "INFO:root:SN: P2800354 | Word Part: PCIe-N572 | DB Part: PCIe-N572 | Status: ✅ MATCH | Qty Check: ✅ Qty OK\n",
      "INFO:root:SN: P3000460 | Word Part: PCIe-N572 | DB Part: PCIe-N572 | Status: ✅ MATCH | Qty Check: ✅ Qty OK\n",
      "INFO:root:SN: P3000461 | Word Part: PCIe-N572 | DB Part: PCIe-N572 | Status: ✅ MATCH | Qty Check: ✅ Qty OK\n",
      "INFO:root:📦 Part Number + Qty Validation:\n",
      "INFO:root:Part: PCIe-N572 | WO: SO-20251009 | Status: ✅ MATCH | Qty Check: ✅ Qty OK\n",
      "INFO:root:\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def process_all_wo_files(folder_path, df_sales_order):\n",
    "    today = datetime.today().date() - timedelta(days=0)\n",
    "    all_results = {}\n",
    "\n",
    "    for root, _, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if file.lower().endswith(\".docx\"):\n",
    "                file_path = os.path.join(root, file)\n",
    "                creation_time = datetime.fromtimestamp(os.path.getctime(file_path)).date()\n",
    "\n",
    "                if creation_time == today:\n",
    "                    logging.info(f\"\\n📄 Processing file: {file}\")\n",
    "\n",
    "                    # --- SN Validation ---\n",
    "                    sn_results = validate_sn_part_matches_via_api(file_path)\n",
    "\n",
    "                    # --- Part Number & Qty Validation ---\n",
    "                    qty_results = validate_part_number_and_qty(file_path, df_sales_order)\n",
    "\n",
    "                    combined_results = {\n",
    "                        \"serial_validation\": sn_results,\n",
    "                        \"part_qty_validation\": qty_results\n",
    "                    }\n",
    "\n",
    "                    all_results[file] = combined_results\n",
    "\n",
    "                    # --- Logging Output ---\n",
    "                    if sn_results:\n",
    "                        logging.info(\"🔍 Serial Number Validation:\")\n",
    "                        for r in sn_results:\n",
    "                            logging.info(f\"SN: {r['serial_number']} | Word Part: {r['word_part']} | \"\n",
    "                                         f\"DB Part: {r['db_part']} | Status: {r['status']} | \"\n",
    "                                         f\"Qty Check: {r['qty_check']}\")\n",
    "                    else:\n",
    "                        logging.warning(\"⚠️ No SN results.\")\n",
    "\n",
    "                    if qty_results:\n",
    "                        logging.info(\"📦 Part Number + Qty Validation:\")\n",
    "                        for r in qty_results:\n",
    "                            logging.info(f\"Part: {r['word_part']} | WO: {r.get('WO_Number')} | \"\n",
    "                                         f\"Status: {r['status']} | Qty Check: {r['qty_match']}\")\n",
    "                    else:\n",
    "                        logging.warning(\"⚠️ No Part/QTY results.\")\n",
    "\n",
    "                    logging.info(\"\\n\" + \"-\"*60 + \"\\n\")\n",
    "\n",
    "    return all_results\n",
    "\n",
    "def normalize(s):\n",
    "    \"\"\"Standardize part number or component string for reliable comparison.\"\"\"\n",
    "    return str(s).strip().upper().replace(\"-\", \"\").replace(\" \", \"\")\n",
    "\n",
    "\n",
    "\n",
    "# Run validations\n",
    "folder = r\"C:\\Users\\Admin\\OneDrive - neousys-tech\\Share NTA Warehouse\\02 Work Order- Word file\\Work Order 2025\\Work Order 2025-08\"\n",
    "results = process_all_wo_files(folder, df_sales_order)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debug\n",
    "-\n",
    "-\n",
    "-\n",
    "-\n",
    "-\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Go Through WO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SN: P2900820 | Word: Nuvo-9006DE | DB: Nuvo-9006DE | ✅ MATCH |  ✅ Qty OK\n",
      "SN: P2900821 | Word: Nuvo-9006DE | DB: Nuvo-9006DE | ✅ MATCH |  ✅ Qty OK\n",
      "SN: P2900824 | Word: Nuvo-9006DE | DB: Nuvo-9006DE | ✅ MATCH |  ✅ Qty OK\n",
      "SN: P2900825 | Word: Nuvo-9006DE | DB: Nuvo-9006DE | ✅ MATCH |  ✅ Qty OK\n",
      "SN: P2900826 | Word: Nuvo-9006DE | DB: Nuvo-9006DE | ✅ MATCH |  ✅ Qty OK\n",
      "SN: P2900827 | Word: Nuvo-9006DE | DB: Nuvo-9006DE | ✅ MATCH |  ✅ Qty OK\n",
      "SN: U3XL329400497, X336H792 | Word: i9-14900 | DB: i9-14900 | ✅ MATCH |  ✅ Qty OK\n",
      "SN: U3EP689002733, X344P526 | Word: i9-14900 | DB: i9-14900 | ✅ MATCH |  ✅ Qty OK\n",
      "SN: M4137QR803356, L437F823 | Word: i9-14900 | DB: i9-14900 | ✅ MATCH |  ✅ Qty OK\n",
      "SN: U4GB082300134, X427M472 | Word: i9-14900 | DB: i9-14900 | ✅ MATCH |  ✅ Qty OK\n",
      "SN: U4XN682803199, X412N124 | Word: i9-14900 | DB: i9-14900 | ✅ MATCH |  ✅ Qty OK\n",
      "SN: U4WH748303353, X412N186 | Word: i9-14900 | DB: i9-14900 | ✅ MATCH |  ✅ Qty OK\n",
      "SN: NCA12407090430054 | Word: DDR5-16GB-WT48-IK | DB: None | ❓ NOT FOUND |  ✅ Qty OK\n",
      "SN: NCA12407090430056 | Word: DDR5-16GB-WT48-IK | DB: None | ❓ NOT FOUND |  ✅ Qty OK\n",
      "SN: NCA12407090430058 | Word: DDR5-16GB-WT48-IK | DB: None | ❓ NOT FOUND |  ✅ Qty OK\n",
      "SN: NCA12407090430080 | Word: DDR5-16GB-WT48-IK | DB: None | ❓ NOT FOUND |  ✅ Qty OK\n",
      "SN: NCA12407090430055 | Word: DDR5-16GB-WT48-IK | DB: None | ❓ NOT FOUND |  ✅ Qty OK\n",
      "SN: NCA12407090430057 | Word: DDR5-16GB-WT48-IK | DB: None | ❓ NOT FOUND |  ✅ Qty OK\n",
      "SN: NCA12407090430075 | Word: DDR5-16GB-WT48-IK | DB: None | ❓ NOT FOUND |  ✅ Qty OK\n",
      "SN: NCA12407090430074 | Word: DDR5-16GB-WT48-IK | DB: None | ❓ NOT FOUND |  ✅ Qty OK\n",
      "SN: NCA12407090430077 | Word: DDR5-16GB-WT48-IK | DB: None | ❓ NOT FOUND |  ✅ Qty OK\n",
      "SN: NCA12407090430076 | Word: DDR5-16GB-WT48-IK | DB: None | ❓ NOT FOUND |  ✅ Qty OK\n",
      "SN: NCA12407090430079 | Word: DDR5-16GB-WT48-IK | DB: None | ❓ NOT FOUND |  ✅ Qty OK\n",
      "SN: NCA12407090430051 | Word: DDR5-16GB-WT48-IK | DB: None | ❓ NOT FOUND |  ✅ Qty OK\n",
      "SN: J392840524 | Word: M.280-SSD-512GB-PCIe44-TLC5WT-TD | DB: M.280-SSD-512GB-PCIe44-TLC5WT-TD | ✅ MATCH |  ✅ Qty OK\n",
      "SN: J392840514 | Word: M.280-SSD-512GB-PCIe44-TLC5WT-TD | DB: M.280-SSD-512GB-PCIe44-TLC5WT-TD | ✅ MATCH |  ✅ Qty OK\n",
      "SN: J392840512 | Word: M.280-SSD-512GB-PCIe44-TLC5WT-TD | DB: M.280-SSD-512GB-PCIe44-TLC5WT-TD | ✅ MATCH |  ✅ Qty OK\n",
      "SN: J392840519 | Word: M.280-SSD-512GB-PCIe44-TLC5WT-TD | DB: M.280-SSD-512GB-PCIe44-TLC5WT-TD | ✅ MATCH |  ✅ Qty OK\n",
      "SN: J392840482 | Word: M.280-SSD-512GB-PCIe44-TLC5WT-TD | DB: M.280-SSD-512GB-PCIe44-TLC5WT-TD | ✅ MATCH |  ✅ Qty OK\n",
      "SN: J392840045 | Word: M.280-SSD-512GB-PCIe44-TLC5WT-TD | DB: M.280-SSD-512GB-PCIe44-TLC5WT-TD | ✅ MATCH |  ✅ Qty OK\n",
      "SN: P2900820 | Word: PCIe-PoE454at | DB: Nuvo-9006DE | ❌ MISMATCH |  ✅ Qty OK\n",
      "SN: P2900821 | Word: PCIe-PoE454at | DB: Nuvo-9006DE | ❌ MISMATCH |  ✅ Qty OK\n",
      "SN: P2900824 | Word: PCIe-PoE454at | DB: Nuvo-9006DE | ❌ MISMATCH |  ✅ Qty OK\n",
      "SN: P2900825 | Word: PCIe-PoE454at | DB: Nuvo-9006DE | ❌ MISMATCH |  ✅ Qty OK\n",
      "SN: P2900826 | Word: PCIe-PoE454at | DB: Nuvo-9006DE | ❌ MISMATCH |  ✅ Qty OK\n",
      "SN: P2900827 | Word: PCIe-PoE454at | DB: Nuvo-9006DE | ❌ MISMATCH |  ✅ Qty OK\n",
      "SN: SC482G6289 | Word: PA-280W-ET3 | DB: PA-280W-ET3 | ✅ MATCH |  ✅ Qty OK\n",
      "SN: SC482G6286 | Word: PA-280W-ET3 | DB: PA-280W-ET3 | ✅ MATCH |  ✅ Qty OK\n",
      "SN: SC482G6284 | Word: PA-280W-ET3 | DB: PA-280W-ET3 | ✅ MATCH |  ✅ Qty OK\n",
      "SN: SC482G6288 | Word: PA-280W-ET3 | DB: PA-280W-ET3 | ✅ MATCH |  ✅ Qty OK\n",
      "SN: SC482G6285 | Word: PA-280W-ET3 | DB: PA-280W-ET3 | ✅ MATCH |  ✅ Qty OK\n",
      "SN: SC482G6291 | Word: PA-280W-ET3 | DB: PA-280W-ET3 | ✅ MATCH |  ✅ Qty OK\n"
     ]
    }
   ],
   "source": [
    "file_path = r\"C:\\Users\\Admin\\OneDrive - neousys-tech\\Share NTA Warehouse\\02 Work Order- Word file\\Work Order 2025\\Work Order 2025-08\\WO08-20250893r-RDI Technologies.docx\"\n",
    "\n",
    "# First, extract product details from the Word document\n",
    "product_details = extract_product_details_from_word(file_path)\n",
    "\n",
    "# Now pass the file_path to the validation function\n",
    "results = validate_sn_part_matches_via_api(file_path)\n",
    "\n",
    "# Print the results\n",
    "for r in results:\n",
    "    print(f\"SN: {r['serial_number']} | Word: {r['word_part']} | DB: {r['db_part']} | {r['status']} |  {r['qty_check']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update changes in excel to Supabase (IN# = IN250225004)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Replaced invoice IN250225004 with 302 rows from Excel.\n"
     ]
    }
   ],
   "source": [
    "# def replace_invoice_rows(file_path, invoice_no):\n",
    "#     import pandas as pd\n",
    "\n",
    "#     # Load and clean the Excel\n",
    "#     df = pd.read_excel(file_path, sheet_name=\"Receiving\")\n",
    "#     df = df.rename(columns={\n",
    "#         'Date': 'entry_date',\n",
    "#         'Inv# ': 'invoice_number',\n",
    "#         'Box #': 'box_number',\n",
    "#         'POD#': 'pod_number',\n",
    "#         'Part#': 'part_number',\n",
    "#         'SN#': 'serial_number',\n",
    "#         'QTY': 'quantity'\n",
    "#     })\n",
    "\n",
    "#     df['quantity'] = df['quantity'].fillna(1).astype(float)\n",
    "#     df['entry_date'] = pd.to_datetime(df['entry_date'], errors='coerce')\n",
    "\n",
    "#     string_cols = ['invoice_number', 'box_number', 'pod_number', 'part_number', 'serial_number']\n",
    "#     for col in string_cols:\n",
    "#         df[col] = df[col].astype(str).str.strip()\n",
    "\n",
    "#     # Filter only rows for the target invoice\n",
    "#     df_target = df[df['invoice_number'] == invoice_no].copy()\n",
    "\n",
    "#     with app.app_context():\n",
    "#         with db.engine.begin() as conn:\n",
    "#             # Delete existing rows for that invoice\n",
    "#             conn.execute(\n",
    "#                 db.text(\"DELETE FROM receiving_log WHERE invoice_number = :inv\"),\n",
    "#                 {\"inv\": invoice_no}\n",
    "#             )\n",
    "#             # Insert the cleaned rows from Excel\n",
    "#             df_target.to_sql('receiving_log', conn, if_exists='append', index=False)\n",
    "\n",
    "#     print(f\"✅ Replaced invoice {invoice_no} with {len(df_target)} rows from Excel.\")\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     replace_invoice_rows(\n",
    "#         r\"C:\\Users\\Admin\\OneDrive - neousys-tech\\Share NTA Warehouse\\01 Incoming\\Receiving Log_ZC.xlsm\",\n",
    "#         \"IN250225004\"\n",
    "#     )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse PDF \n",
    "#### (Before 20250187: [None, None, 'P.O. No.', 'Ship Date', None, 'Expected L/T', None] )\n",
    "#### (After 20250187: ['Item', 'Description', 'Site', 'Ordered', 'To Pick'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WO: 20250957r\n",
      "Customer: CACI Federal\n",
      "PO: P000214574\n",
      "                         Extracted Ordered\n",
      "4                       SEMIL-2007       6\n",
      "5                         i9-14900       6\n",
      "8                  DDR5-32GB-48-SM      12\n",
      "9   M.280-SSD-2TB-PCIe44-TLC5WT-TD       6\n",
      "11            Cblkit-M12-SEMIL2000       6\n",
      "15          Cbl-TpCPlug-UTpCF-50CM       6\n",
      "16           Cbl-TpCPlug-U3TA-50CM       6\n",
      "17              Cbl-TpCPlug-DPM-1M       6\n",
      "19                 PA-280W-CW6P-2P       6\n",
      "20                Cbl-PC-TW-180CM1       6\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "Path = r\"\\\\Quickbook2024\\d\\Drive D\\QuickBooks\\3- Year 2025\\Work Order- WO\\WO-20250957r_CACI Federal_PO_P000214574_(2-2).pdf\"\n",
    "\n",
    "filename = os.path.basename(Path)\n",
    "\n",
    "Cleaned_header = ['Item', 'Description', 'Site', 'Ordered', 'To Pick']\n",
    "\n",
    "with pdfplumber.open(Path) as pdf:\n",
    "    match = re.search(r\"WO-(\\d{6,8}[a-zA-Z]?)_([A-Za-z0-9\\s&\\-]+)_PO_(P\\d+)\", filename)\n",
    "    if match:\n",
    "        wo = match.group(1)             # '20250957r'\n",
    "        customer = match.group(2)       # 'CACI Federal'\n",
    "        po = match.group(3)             # 'P000214574'\n",
    "\n",
    "        print(\"WO:\", wo)\n",
    "        print(\"Customer:\", customer)\n",
    "        print(\"PO:\", po)\n",
    "    else:\n",
    "        print(\"Pattern not matched.\")\n",
    "\n",
    "\n",
    "    tables = []\n",
    "    for page in pdf.pages:\n",
    "        table = page.extract_table()\n",
    "        if not table:\n",
    "            continue\n",
    "\n",
    "        header = table[0]\n",
    "        cleaned_raw_header = [str(col).strip() if col else \"\" for col in header]\n",
    "\n",
    "        if cleaned_raw_header == Cleaned_header:\n",
    "            # Standard case\n",
    "            df = pd.DataFrame(table[1:], columns=Cleaned_header)\n",
    "            tables.append(df)\n",
    "\n",
    "        elif 'P.O. No.' in cleaned_raw_header:\n",
    "            try:\n",
    "                po_index = cleaned_raw_header.index('P.O. No.')\n",
    "                # Remove the column from all rows\n",
    "                data_rows = [row[:po_index] + row[po_index+1:] for row in table[1:]]\n",
    "                # Remove from header too\n",
    "                cleaned_alt_header = cleaned_raw_header[:po_index] + cleaned_raw_header[po_index+1:]\n",
    "\n",
    "                # Force assign Cleaned_header if lengths match\n",
    "                if len(Cleaned_header) == len(cleaned_alt_header):\n",
    "                    df = pd.DataFrame(data_rows, columns=Cleaned_header)\n",
    "                    tables.append(df)\n",
    "                else:\n",
    "                    print(\"Header length mismatch after removing 'P.O. No.':\", cleaned_alt_header)\n",
    "            except Exception as e:\n",
    "                print(\"Error processing alternate header table:\", e)\n",
    "\n",
    "        else:\n",
    "            print(\"Unknown table format:\", cleaned_raw_header)\n",
    "\n",
    "\n",
    "# Combine all extracted tables\n",
    "final_df = pd.concat(tables, ignore_index=True)\n",
    "\n",
    "# Strip column names of trailing spaces\n",
    "final_df.columns = final_df.columns.str.strip()\n",
    "\n",
    "# Replace any odd encoding\n",
    "for col in final_df.select_dtypes(include='object').columns:\n",
    "    final_df[col] = final_df[col].map(lambda x: x.replace(\"(cid:95)\", \"_\") if isinstance(x, str) else x)\n",
    "\n",
    "# Filter rows where 'Site' starts with 'WH'\n",
    "filtered_df = final_df[final_df['Site'].astype(str).str.startswith('WH')].copy()\n",
    "\n",
    "def extract_or_default(row):\n",
    "    col1 = row['Item']\n",
    "    col2 = row['Description']\n",
    "    if isinstance(col1, str) and '...' in col1:\n",
    "        if isinstance(col2, str):\n",
    "            match = re.search(r'\\*(.*?)\\n', col2)\n",
    "            if match:\n",
    "                return match.group(1)\n",
    "        return None\n",
    "    else:\n",
    "        return col1\n",
    "\n",
    "filtered_df['Extracted'] = filtered_df.apply(extract_or_default, axis=1)\n",
    "print(filtered_df[['Extracted','Ordered']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('20250923',\n",
       " 'CoastIPC',\n",
       " 'P96448',\n",
       "             Extracted Ordered\n",
       " 3  Nuvo-9166GC-PoE-UL       1\n",
       " 4           i9-13900E       1)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_pdf_data(pdf_path):\n",
    "    filename = os.path.basename(pdf_path)\n",
    "    name_without_ext = filename.replace('.pdf', '')\n",
    "    parts = name_without_ext.split('_')\n",
    "    wo_match = re.match(r\"WO-(\\d{8}[a-zA-Z]?)\", parts[0])\n",
    "    if wo_match and len(parts) >= 3 and parts[-2] == \"PO\":\n",
    "        wo = wo_match.group(1)\n",
    "        customer = \"_\".join(parts[1:-2]).replace('_', ' ')  # Handles multi-word customer\n",
    "        po = parts[-1]\n",
    "    \n",
    "\n",
    "    tables = []\n",
    "    cleaned_header = ['Item', 'Description', 'Site', 'Ordered', 'To Pick']\n",
    "    \n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            table = page.extract_table()\n",
    "            if not table:\n",
    "                continue\n",
    "            header = table[0]\n",
    "            cleaned_raw_header = [str(col).strip() if col else \"\" for col in header]\n",
    "\n",
    "            if cleaned_raw_header == cleaned_header:\n",
    "                df = pd.DataFrame(table[1:], columns=cleaned_header)\n",
    "                tables.append(df)\n",
    "            elif 'P.O. No.' in cleaned_raw_header:\n",
    "                try:\n",
    "                    po_index = cleaned_raw_header.index('P.O. No.')\n",
    "                    data_rows = [row[:po_index] + row[po_index+1:] for row in table[1:]]\n",
    "                    cleaned_alt_header = cleaned_raw_header[:po_index] + cleaned_raw_header[po_index+1:]\n",
    "                    if len(cleaned_header) == len(cleaned_alt_header):\n",
    "                        df = pd.DataFrame(data_rows, columns=cleaned_header)\n",
    "                        tables.append(df)\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "    if not tables:\n",
    "        return wo, customer, po, pd.DataFrame()\n",
    "    \n",
    "    final_df = pd.concat(tables, ignore_index=True)\n",
    "    final_df.columns = final_df.columns.str.strip()\n",
    "\n",
    "    # Filter after concat\n",
    "    final_df = final_df[final_df['Site'].astype(str).str.startswith('WH')].copy()\n",
    "\n",
    "    def extract_or_default(row):\n",
    "        col1 = row['Item']\n",
    "        col2 = row['Description']\n",
    "        if isinstance(col1, str) and '...' in col1:\n",
    "            if isinstance(col2, str):\n",
    "                match = re.search(r'\\*(.*?)\\n', col2)\n",
    "                if match:\n",
    "                    return match.group(1)\n",
    "            return None\n",
    "        else:\n",
    "            return col1\n",
    "\n",
    "    final_df['Extracted'] = final_df.apply(extract_or_default, axis=1)\n",
    "    return wo, customer, po, final_df[['Extracted', 'Ordered']]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df = extract_pdf_data(r\"\\\\Quickbook2024\\d\\Drive D\\QuickBooks\\3- Year 2025\\Work Order- WO\\WO-20250923_CoastIPC_PO_P96448.pdf\")\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Customer</th>\n",
       "      <th>PO</th>\n",
       "      <th>WO</th>\n",
       "      <th>Product Number</th>\n",
       "      <th>Qty</th>\n",
       "      <th>Source File</th>\n",
       "      <th>Flagged</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GPR</td>\n",
       "      <td>1026</td>\n",
       "      <td>SO-20250001</td>\n",
       "      <td>Nuvo-7204VTC</td>\n",
       "      <td>5</td>\n",
       "      <td>WO-20250001_GPR_PO#1026.pdf</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GPR</td>\n",
       "      <td>1026</td>\n",
       "      <td>SO-20250001</td>\n",
       "      <td>i7-8700T</td>\n",
       "      <td>5</td>\n",
       "      <td>WO-20250001_GPR_PO#1026.pdf</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GPR</td>\n",
       "      <td>1026</td>\n",
       "      <td>SO-20250001</td>\n",
       "      <td>DDR4-32GB-WT32-SM</td>\n",
       "      <td>5</td>\n",
       "      <td>WO-20250001_GPR_PO#1026.pdf</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GPR</td>\n",
       "      <td>1026</td>\n",
       "      <td>SO-20250001</td>\n",
       "      <td>SSD-256GB-TLCWT-IK</td>\n",
       "      <td>5</td>\n",
       "      <td>WO-20250001_GPR_PO#1026.pdf</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GPR</td>\n",
       "      <td>1026</td>\n",
       "      <td>SO-20250001</td>\n",
       "      <td>SSD-2TB-TLC5WT-TD</td>\n",
       "      <td>5</td>\n",
       "      <td>WO-20250001_GPR_PO#1026.pdf</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6167</th>\n",
       "      <td>Intelligent Wellhead(Canada)</td>\n",
       "      <td>9001</td>\n",
       "      <td>SO-20251149</td>\n",
       "      <td>i9-13900</td>\n",
       "      <td>6</td>\n",
       "      <td>WO-20251149_Intelligent Wellhead(Canada)_PO_90...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6168</th>\n",
       "      <td>Intelligent Wellhead(Canada)</td>\n",
       "      <td>9001</td>\n",
       "      <td>SO-20251149</td>\n",
       "      <td>GC-RTX4000SFFAda-PNY</td>\n",
       "      <td>6</td>\n",
       "      <td>WO-20251149_Intelligent Wellhead(Canada)_PO_90...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6169</th>\n",
       "      <td>Intelligent Wellhead(Canada)</td>\n",
       "      <td>9001</td>\n",
       "      <td>SO-20251149</td>\n",
       "      <td>DDR5-32GB-48WT-SM</td>\n",
       "      <td>12</td>\n",
       "      <td>WO-20251149_Intelligent Wellhead(Canada)_PO_90...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6170</th>\n",
       "      <td>Intelligent Wellhead(Canada)</td>\n",
       "      <td>9001</td>\n",
       "      <td>SO-20251149</td>\n",
       "      <td>SSD-4TB-TLC5WT-TD</td>\n",
       "      <td>6</td>\n",
       "      <td>WO-20251149_Intelligent Wellhead(Canada)_PO_90...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6171</th>\n",
       "      <td>Intelligent Wellhead(Canada)</td>\n",
       "      <td>9001</td>\n",
       "      <td>SO-20251149</td>\n",
       "      <td>Gpubr-Nuvo9160-01</td>\n",
       "      <td>6</td>\n",
       "      <td>WO-20251149_Intelligent Wellhead(Canada)_PO_90...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6172 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Customer    PO           WO        Product Number  \\\n",
       "0                              GPR  1026  SO-20250001          Nuvo-7204VTC   \n",
       "1                              GPR  1026  SO-20250001              i7-8700T   \n",
       "2                              GPR  1026  SO-20250001     DDR4-32GB-WT32-SM   \n",
       "3                              GPR  1026  SO-20250001    SSD-256GB-TLCWT-IK   \n",
       "4                              GPR  1026  SO-20250001     SSD-2TB-TLC5WT-TD   \n",
       "...                            ...   ...          ...                   ...   \n",
       "6167  Intelligent Wellhead(Canada)  9001  SO-20251149              i9-13900   \n",
       "6168  Intelligent Wellhead(Canada)  9001  SO-20251149  GC-RTX4000SFFAda-PNY   \n",
       "6169  Intelligent Wellhead(Canada)  9001  SO-20251149     DDR5-32GB-48WT-SM   \n",
       "6170  Intelligent Wellhead(Canada)  9001  SO-20251149     SSD-4TB-TLC5WT-TD   \n",
       "6171  Intelligent Wellhead(Canada)  9001  SO-20251149     Gpubr-Nuvo9160-01   \n",
       "\n",
       "     Qty                                        Source File Flagged  \n",
       "0      5                        WO-20250001_GPR_PO#1026.pdf          \n",
       "1      5                        WO-20250001_GPR_PO#1026.pdf          \n",
       "2      5                        WO-20250001_GPR_PO#1026.pdf          \n",
       "3      5                        WO-20250001_GPR_PO#1026.pdf          \n",
       "4      5                        WO-20250001_GPR_PO#1026.pdf          \n",
       "...   ..                                                ...     ...  \n",
       "6167   6  WO-20251149_Intelligent Wellhead(Canada)_PO_90...          \n",
       "6168   6  WO-20251149_Intelligent Wellhead(Canada)_PO_90...          \n",
       "6169  12  WO-20251149_Intelligent Wellhead(Canada)_PO_90...          \n",
       "6170   6  WO-20251149_Intelligent Wellhead(Canada)_PO_90...          \n",
       "6171   6  WO-20251149_Intelligent Wellhead(Canada)_PO_90...          \n",
       "\n",
       "[6172 rows x 7 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Customer</th>\n",
       "      <th>Customer</th>\n",
       "      <th>PO</th>\n",
       "      <th>PO</th>\n",
       "      <th>WO</th>\n",
       "      <th>Product Number</th>\n",
       "      <th>Qty</th>\n",
       "      <th>Source File</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>Boston Scientific</td>\n",
       "      <td></td>\n",
       "      <td>7000580842</td>\n",
       "      <td></td>\n",
       "      <td>SO-20250843</td>\n",
       "      <td>Nuvo-9006DE-PoE-UL</td>\n",
       "      <td>1.0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>Boston Scientific</td>\n",
       "      <td></td>\n",
       "      <td>7000580842</td>\n",
       "      <td></td>\n",
       "      <td>SO-20250843</td>\n",
       "      <td>i9-13900</td>\n",
       "      <td>1.0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>Boston Scientific</td>\n",
       "      <td></td>\n",
       "      <td>7000580842</td>\n",
       "      <td></td>\n",
       "      <td>SO-20250843</td>\n",
       "      <td>DDR5-16GB-48-SM</td>\n",
       "      <td>2.0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357</th>\n",
       "      <td>Boston Scientific</td>\n",
       "      <td></td>\n",
       "      <td>7000580842</td>\n",
       "      <td></td>\n",
       "      <td>SO-20250843</td>\n",
       "      <td>M.280-SSD-256GB-PCIe44-TLC5WT-TD</td>\n",
       "      <td>1.0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358</th>\n",
       "      <td>Boston Scientific</td>\n",
       "      <td></td>\n",
       "      <td>7000580842</td>\n",
       "      <td></td>\n",
       "      <td>SO-20250843</td>\n",
       "      <td>SSD-1TB-TLC5ET-TD</td>\n",
       "      <td>1.0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359</th>\n",
       "      <td>Boston Scientific</td>\n",
       "      <td></td>\n",
       "      <td>7000580842</td>\n",
       "      <td></td>\n",
       "      <td>SO-20250843</td>\n",
       "      <td>Dust Cover-DP</td>\n",
       "      <td>1.0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>Boston Scientific</td>\n",
       "      <td></td>\n",
       "      <td>7000580842</td>\n",
       "      <td></td>\n",
       "      <td>SO-20250843</td>\n",
       "      <td>Dust Cover-COM</td>\n",
       "      <td>4.0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>Boston Scientific</td>\n",
       "      <td></td>\n",
       "      <td>7000580842</td>\n",
       "      <td></td>\n",
       "      <td>SO-20250843</td>\n",
       "      <td>Dust Cover-USB-AF</td>\n",
       "      <td>8.0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>Boston Scientific</td>\n",
       "      <td></td>\n",
       "      <td>7000580842</td>\n",
       "      <td></td>\n",
       "      <td>SO-20250843</td>\n",
       "      <td>Dust Cover-USB-CF</td>\n",
       "      <td>1.0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>Boston Scientific</td>\n",
       "      <td></td>\n",
       "      <td>7000580842</td>\n",
       "      <td></td>\n",
       "      <td>SO-20250843</td>\n",
       "      <td>Dust Cover-DVI</td>\n",
       "      <td>1.0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>Boston Scientific</td>\n",
       "      <td></td>\n",
       "      <td>7000580842</td>\n",
       "      <td></td>\n",
       "      <td>SO-20250843</td>\n",
       "      <td>Dust Cover-VGA-HDB15M</td>\n",
       "      <td>1.0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>Boston Scientific</td>\n",
       "      <td></td>\n",
       "      <td>7000580842</td>\n",
       "      <td></td>\n",
       "      <td>SO-20250843</td>\n",
       "      <td>Dust Cover-RJ45</td>\n",
       "      <td>6.0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>366</th>\n",
       "      <td>Boston Scientific</td>\n",
       "      <td></td>\n",
       "      <td>7000580842</td>\n",
       "      <td></td>\n",
       "      <td>SO-20250843</td>\n",
       "      <td>DINRAIL-O</td>\n",
       "      <td>1.0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367</th>\n",
       "      <td>Boston Scientific</td>\n",
       "      <td></td>\n",
       "      <td>7000580842</td>\n",
       "      <td></td>\n",
       "      <td>SO-20250843</td>\n",
       "      <td>PA-280W-ET3</td>\n",
       "      <td>1.0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368</th>\n",
       "      <td>Boston Scientific</td>\n",
       "      <td></td>\n",
       "      <td>7000580842</td>\n",
       "      <td></td>\n",
       "      <td>SO-20250843</td>\n",
       "      <td>Cbl-PC-TW-180CM1</td>\n",
       "      <td>1.0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Customer Customer          PO PO           WO  \\\n",
       "354  Boston Scientific           7000580842     SO-20250843   \n",
       "355  Boston Scientific           7000580842     SO-20250843   \n",
       "356  Boston Scientific           7000580842     SO-20250843   \n",
       "357  Boston Scientific           7000580842     SO-20250843   \n",
       "358  Boston Scientific           7000580842     SO-20250843   \n",
       "359  Boston Scientific           7000580842     SO-20250843   \n",
       "360  Boston Scientific           7000580842     SO-20250843   \n",
       "361  Boston Scientific           7000580842     SO-20250843   \n",
       "362  Boston Scientific           7000580842     SO-20250843   \n",
       "363  Boston Scientific           7000580842     SO-20250843   \n",
       "364  Boston Scientific           7000580842     SO-20250843   \n",
       "365  Boston Scientific           7000580842     SO-20250843   \n",
       "366  Boston Scientific           7000580842     SO-20250843   \n",
       "367  Boston Scientific           7000580842     SO-20250843   \n",
       "368  Boston Scientific           7000580842     SO-20250843   \n",
       "\n",
       "                       Product Number  Qty Source File  \n",
       "354                Nuvo-9006DE-PoE-UL  1.0              \n",
       "355                          i9-13900  1.0              \n",
       "356                   DDR5-16GB-48-SM  2.0              \n",
       "357  M.280-SSD-256GB-PCIe44-TLC5WT-TD  1.0              \n",
       "358                 SSD-1TB-TLC5ET-TD  1.0              \n",
       "359                     Dust Cover-DP  1.0              \n",
       "360                    Dust Cover-COM  4.0              \n",
       "361                 Dust Cover-USB-AF  8.0              \n",
       "362                 Dust Cover-USB-CF  1.0              \n",
       "363                    Dust Cover-DVI  1.0              \n",
       "364             Dust Cover-VGA-HDB15M  1.0              \n",
       "365                   Dust Cover-RJ45  6.0              \n",
       "366                         DINRAIL-O  1.0              \n",
       "367                       PA-280W-ET3  1.0              \n",
       "368                  Cbl-PC-TW-180CM1  1.0              "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def reorder_df_out_by_output(output_df: pd.DataFrame, df_out: pd.DataFrame) -> pd.DataFrame:\n",
    "    # 1) Build the reference order from output_df (the order you trust)\n",
    "    ref = output_df.copy()\n",
    "    ref['__pos_out'] = ref.groupby('WO').cumcount()                 # position within WO\n",
    "    ref['__occ'] = ref.groupby(['WO','Product Number']).cumcount()  # occurrence index for duplicates\n",
    "    ref_key = ref[['WO','Product Number','__occ','__pos_out']]\n",
    "\n",
    "    # 2) On df_out, tag each duplicate with its own occurrence index\n",
    "    tgt = df_out.copy()\n",
    "    tgt['__occ'] = tgt.groupby(['WO','Product Number']).cumcount()\n",
    "\n",
    "    # 3) Merge positions from output_df to df_out rows (match by WO + Product + occurrence)\n",
    "    merged = tgt.merge(ref_key, on=['WO','Product Number','__occ'], how='left')\n",
    "\n",
    "    # 4) For rows not present in output_df, keep their original within-WO order but push them after the matched ones\n",
    "    merged['__fallback'] = merged.groupby('WO').cumcount()\n",
    "    merged['__pos_out'] = merged['__pos_out'].fillna(np.inf)\n",
    "\n",
    "    # 5) Final order: by WO, then by output_df position; if missing, by original order\n",
    "    ordered = (merged\n",
    "               .sort_values(['WO','__pos_out','__fallback'])\n",
    "               .drop(columns=['__occ','__pos_out','__fallback'])\n",
    "               .reset_index(drop=True))\n",
    "    return ordered\n",
    "\n",
    "\n",
    "# Usage:\n",
    "final_df = reorder_df_out_by_output(output_df, df_out)\n",
    "\n",
    "\n",
    "# your specific mappings (only exact string-to-string)\n",
    "mappings = {\n",
    "    'M.280-SSD-256GB-PCIe44-TLC5WT-T': 'M.280-SSD-256GB-PCIe44-TLC5WT-TD',\n",
    "    'M.280-SSD-512GB-PCIe44-TLC5WT-T': 'M.280-SSD-512GB-PCIe44-TLC5WT-TD',\n",
    "    'M.242-SSD-256GB-PCIe34-TLC5WT-T': 'M.242-SSD-256GB-PCIe34-TLC5WT-TD',\n",
    "    'M.242-SSD-512GB-PCIe34-TLC5WT-T': 'M.242-SSD-512GB-PCIe34-TLC5WT-TD',\n",
    "    'Cblkit-FP-NRU-230V-AWP_NRU-240S': 'Cblkit-FP-NRU-230V-AWP_NRU-240S-AWP',\n",
    "}\n",
    "\n",
    "final_df['Product Number'] = final_df['Product Number'].replace(mappings)\n",
    "\n",
    "\n",
    "# ✅ Now final_df has Product Number overridden from df_out when WO matches and value differs\n",
    "final_df[final_df['WO'] == 'SO-20250843']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
